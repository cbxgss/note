# 1.绪论

## 1.1简述

-   人工智能
-   图灵测试
-   人工智能的研究领域
    -   让计算机具有人类的智能
        -   机器感知: 视觉、语音信息处理、自然语言处理
        -   学习: 数据挖掘、机器学习
        -   决策: 规划、推理
-   什么是机器学习: 让计算机从数据中进行自动学习，得到某种知识

## 1.2神经网络与深度学习的发展

-   模型提出: 感知机
-   冰河期
-   反向传播算法引起的复兴: 多层感知器，卷积神经网络
-   流行度降低: SVM
-   深度学习的崛起

## 1.3深度学习的应用

# 2.神经网络基础

## 2.1感知机

### 2.1.1模型原理

-   背景

-   **模型**: 模拟神经元
    $$
    f(x)=sign(w^Tx+b)
    $$

    -   输入: 接受的信号
    -   输出: 神经元的状态(兴奋/抑制)
    -   线性二分类器

-   应用
    -   垃圾邮件
    -   文档分类

### 2.1.2优化算法

-   **模型优化**

    -   对于一个错误分类的数据$(x_i, y_i)$
        $$
        y_i(w^Tx+b) < 0
        $$

    -   点到直线的距离
        $$
        \frac{\abs{w^Tx+b}}{\abs{w}} = -\frac{(w^Tx+b)y_i}{\abs{w}}
        $$

    -   优化目标: 减小所有误分类点到超平面的距离
        $$
        \mathop{min}\limits_{w,b} L(w,b) = -\frac{1}{\abs{w}} \sum\limits_{x_i \in M} y_i(w^Tx+b)
        $$

    -   优化方法: 梯度下降
        $$
        w = w - k \nabla_w L(W, b) \\
        b = b - k \nabla_b L(W, b) \\
        k \in (0, 1]
        $$

        -   梯度计算
            $$
            \nabla_w L(W, b) = - \sum\limits_{x_i \in M} y_ix_i \\
            \nabla_b L(W, b) = - \sum\limits_{x_i \in M} y_i \\
            $$

    -   模型优化
        $$
        w = w + k y_i x_i \\
        b = b + k y_i
        $$

-   **模型收敛性**

    -   定理
        
        -   对于任意一个线性可分的数据集，存在 $\abs{\hat{w}^*} = 1$ 的超平面 $S^*$ 能将所有样本正确分类，且存在一个常数 $ \gamma >0$，使得 
            $$
            y_i({\hat{w}^*}^Tx)>= \gamma
            $$
        
        -   训练数据集中最大的特征向量的模
            $$
            R = \mathop{max}\limits_{1 \le i \le N} \abs{x_i}
            $$
        
        -   训练时更新的次数
            $$
            k \le (\frac{R}{\gamma})^2
            $$
        
    -   证明
        -   线性可分则存在超平面可以完全分开
            $$
            y_i(w_{opt} \cdot x_i) \ge 0, \; \abs{w_{opt}} = 1
            $$

        -   则存在
            $$
            \gamma = \min\limits_{i} \{ y_i (w_{opt} \cdot x_i + b_{opt}) \}
            $$
            
        -   
    
-   **多分类扩展**
    -   一对多
    -   投票
    -   **softmax**: 一类一个score，取最大的

-   **缺点与限制**
    -   xor 问题
    -   不具备处理非线性可分问题的能力

## 2.2前馈神经网络

### 2.3.1模型原理

#### 2.3.1.1感知器和人工神经元

-   **感知器**
    -   激活函数 $sign$ 函数不可导，影响参数学习
-   **人工神经元**: 修改了**激活函数**
    -   sigmoid型
        -   logistic: $\sigma(x) = \frac{1}{1+e^x}$
        -   Tanh: $\tanh(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}$
    -   Relu型
        -   $ReLU(x) = max(0, x)$
        -   $LeakyReLU(x) = max(0, x) + min(0, \gamma x)$
        -   $PReLu(x) = max(0, x) + min(0, \gamma_i x)$
        -   $ELU(x) = max(0,x) + min(0, \gamma(e^x -1))$
        -   $SoftPlus(x) = log(1+e^x)$

#### 2.3.1.2前馈神经网络

-   **结构**

    ![image-20211208130913398](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211208130913398-16389401547851.png)
    $$
    z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\
    a^{(l)} = f_{l}(z^{(l)})
    $$

    -   $z^{(l)} \in \mathbb{R}^{M_l}$: 第 $l$ 层神经元的净输入

    -   $a^{(l)} \in \mathbb{R}^{M_l}$: 第 $l$ 层神经元的输入

    -   $f_l()$: 第 $l$ 层的激活函数

    -   基本符号

        -   $L$: 神经网络层数

        -   $M_i$: 第 $l$ 层神经元的个数

    -   神经网络参数
        -   $W^{(l)} \in \mathbb{R}^{M_i \times M_{i-1}}$: 第 $l-1$ 到第 $l$ 层的权重矩阵
        -   $b^{(l)} \in \mathbb{R}^{M_i}$: 第 $l-1$ 到第 $l$ 层的偏置

-   **优势**

    -   解决异或问题
    -   解决其他非线性问题
    -   **通用近似定理**

### 2.3.2反向传播BP

-   前馈神经网络的**任务定义**

    -   用一个函数逼近一个复杂的条件分布
    -   多分类问题: 输出层不只一个神经元，而是多个神经元，然后 softmax

-   **优化目标**
    $$
    \min R(W,b) = \frac{1}{N} \sum\limits_{n=1}^{N} L(y^{(n)}, \hat{y}^{(n)}) + \frac{1}{2} \lambda \abs{W}^2_F
    $$

    -   $y$: 真值
    -   $\hat{y}$: 网络输出
    -   $N$: 样本量

-   **参数学习**
    $$
    W^{(l)} = W^{(l)} - \alpha \frac{\partial R(W, b)}{\partial W^{(l)}} \\
    b^{(l)} = b^{(l)} - \alpha \frac{\partial R(W, b)}{\partial b^{(l)}}
    $$

    -   其中
        $$
        \begin{split}
        \frac{\partial R(W, b)}{\partial w_{ij}^{(l)}}
        & = \frac{\partial L(y, \hat{y})}{\partial w_{ij}^{(l)}} + \lambda \abs{W} \\
        & = \frac{\partial L(y, \hat{y})}{\partial z^{(l)}} \frac{\partial z^{(l)}}{\partial w_{ij}^{(l)}} + \lambda \abs{W} \\
        \end{split}
        $$

        -   有
            $$
             \frac{\partial L(y, \hat{y})}{\partial w_{ij}^{(l)}} = \delta_i^{(l)} a_j^{(l-1)} \\
              \frac{\partial L(y, \hat{y})}{\partial W^{(l)}} = \delta_i^{(l)} a^{(l-1)}
            $$

        -   有
            $$
            \frac{\partial L(y, \hat{y})}{\partial z^{(l)}}  = \delta^{(l)}
            $$
            

### 2.3.3前馈神经网络的实现

## 2.3卷积神经网络

-   前馈神经网络的缺点
    -   过拟合
    -   权重矩阵参数很多
    -   很难提取局部不变特征

### 卷积层

-   **一维卷积**

    -   **局部(稀疏)连接**

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144239726.png" alt="image-20211122144239726" style="zoom: 80%;" />

    -   **权重共享**(用一个卷积核 / 滤波器)

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144314626.png" alt="image-20211122144314626" style="zoom:80%;" />

    -   **等变表示**

        -   等变: 一个函数，输入以某种方式改变，输出也跟着以同样方式变

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144458360.png" alt="image-20211122144458360" style="zoom: 67%;" />

    -   **不同卷积核提取信号序列中的不同特征**

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144650788.png" alt="image-20211122144650788" style="zoom:80%;" />

    -   其他扩展

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144745013.png" alt="image-20211122144745013" style="zoom:80%;" />

-   **二维卷积**

    -   卷积核

        <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122144839394.png" alt="image-20211122144839394" style="zoom:80%;" />

    -   用于图像处理

    -   二维卷积扩展

        -   步长和填充

            <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122145025224.png" alt="image-20211122145025224" style="zoom: 80%;" />

        -   空洞卷积

            <img src="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211122145053626.png" alt="image-20211122145053626" style="zoom:67%;" />

    -   多通道卷积扩展(最常见实用): 利用不同的卷积核提取不同的信息，取得更好的结果
    
        ![image-20220126000119850](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220126000119850.png)

### 非线性激活层

-   激活函数

### 降采样(池化)层

-   max pooling
-   mean pooling
-   降低输出对形状的敏感度

### 全局聚合层

最后放一个全连接层(特征聚合与结果预测)

### 卷积神经网络结构

卷积层，池化层，全连接层交叉堆叠

![image-20211124153751802](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211124153751802.png)

### LeNet-5

![image-20211124155210822](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211124155210822.png)

-   输入: 32 \* 32 图像
-   结构
    -   2(3) 卷积层 $C_x$
    -   3(2) 全连接层 $S_x$
    -   2 降采样层 $F_x$
-   $C_1$: 通道数 = 6，卷积核 5\* 5 (步长1，无填充)
    -   $[32 * 32] \rightarrow 6[28 * 28]$
-   $S_2$: 通道数 = 6，用 2 \* 2 步长为2的池化
    -   $6[28 * 28] \rightarrow 6[14 * 14]$
-   $C_3$: 通道数 = 16，卷积核 5 \* 5 (步长1，无填充)
    -   $6[14 * 14] \rightarrow 16[10 * 10]$
-   $S_4$: 通道数 = 16，用 2 \* 2 步长为2的池化
    -   $16[10 * 10] \rightarrow 16[5 * 5]$
-   $C_5$(可以看做全连接层): 通道数 = 120，卷积核 5 \* 5 (步长1，无填充)
    -   $[5 * 5] \rightarrow 120[1 * 1]$
-   $F_6$: 全连接层，输入120个神经元，输出84个神经元

# 深层卷积神经网络

## 架构发展

LeNet -> AlexNet -> ZF-Net -> VGG-Net -> ResNet -> DenseNet ->SE-Net

### LeNet

-   5层

### AlexNet

-   11 \* 11
-   梯度消失
    -   加速收敛: sigmoid -> relu
-   过拟合
    -   数据增广
    -   Dropout: 训练时随机让一些连接失效(测试时不能用)
        -   本质: 随机去掉训练图像的一些信息
-   问题
    -   第一层偏重低频和高频信息
    -   第一二层信息振铃现象严重

### ZF-Net

### VGG

-   LRn不重要
-   初始化方式很重要
    -   用浅层网络初始化深层网络: 渐进式加层(但是更深的网络训练很困难)
    -   均匀分布初始化方式

### ResNet

-   100+层神经网络
-   梯度消失
    -   残差链接
-   用全局均值池化代替全连接层

### DenseNet

-   对残差链接dropout
-   把x扩展到之前更多层的x
    -   分成n组，每组k层，组内残差链接 n * Ck2

### SE-Net

-   对残差链接的dropout时，0 or 1 -> [0, 1]

## Inception

### Inception-V1

-   结构

    ![img](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/1456303-20190504162301047-898768446.png)

-   问题: Cout连接

    ![img](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/1456303-20190504162327115-2119269106.png)

    -   能利用不同大小的滤波器

    -   扩展了CNN宽度

    -   inception本身就是一个神经网络(神经元是网络)

-   GoogleNet
    -   需要2个辅助分类器

### Inception-V2

BN-Inception

-   把 5\*5改成两层3\*3
-   BN算法
    -   白化处理(归一化)
    -   线性变换: 避免前一层学到的东西由于白化丢失

### Inception-V3

-   分解大卷积核
-   CNN架构深度和宽度更平衡
-    
-   输入图片更大
-   lable one-hot[1, 0, 0] -> 平滑[0.8, 0.1, 0.1]

### Inception-V4

Inception-ResNet

## 轻量化深层神经网络&神经网络架构搜索

### 轻量化深层神经网络

-   MobileNet
    -   v1
        -   深度可分离卷积
        -   两个参数: 宽度控制参数(通道数) $\alpha$，分辨率控制参数(图片大小) $\rho$
    -   v2
        -   倒残差
        -   维度缩减层的输出不接非线性层: 减少信息丢失
        -   Relu6
    -   v3
        -   注意力机制
        -   用 swish 非线性激活函数代替 ReLU
            -   用分段线性函数代替 $\sigma$ 函数
        -   删除一些网络结构
    
-   ShuffleNet

    -   V1

        -   通道交替代替点组卷积

            ![CNN模型之ShuffleNet](%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-cb29dd7401981a2f7efc10bffe3898f8_1440w.jpg)

    -   v2
        -   输入和输出通道数相同
        -   不要过多的组卷积
        -   减少网络内部的碎片化操作
        -   减少元素级计算操作

### 深层神经网络

-   Darts

### 深层神经网络架构搜索

## 深层神经网络优化

-   **随机梯度下降**SGD: 一个样本
-   **小批量随机梯度下降** Mini-Batch SGD
    -   批量大小不会影响随机梯度的期望
    -   会影响方差
        -   越小越不稳定，需要更小的学习率
-   改进的SGD
    -   动态学习率
        -   1
        -   Adagred
        -   RMSprop: 将Adagrad里面的 $G_t$ 累加 增加一个 $\beta$ 衰减
        -   Adadelta: 没有初始人为设置的学习率 $\alpha$，用之前的梯度缩减累加当学习率
    -   更新策略
        -   动量法: 用衰减累计动量代替梯度
        -   Nesterow加速梯度，先算一下，然后再计算梯度，然后重新计算
    -   Adam算法: RMSprop + 动量法
-   优化算法总结
-   最近进展
    -   二阶优化算法
    -   分布式二阶优化算法



-   批次归一化 BN
    -   白化
    -   线性变化
-   逐层归一化

# 循环神经网络RNN

-   延迟器: 使用待自反馈的神经元

-   隐藏层模型
    $$
    h_t = f(Wh_{t-1} + Ux_t + b)
    $$

    -   $x_t$: 次序输入
        -   不同于前馈神经网络，是一次性输入
        -   前馈神经网络有多少神经元就输入几个，但是RNN顺序可变

-   LSTM

# 生成对抗网络GAN

-   最大似然估计

# 图神经网络

# 大作业

-   课程设计
    -   至少三种
