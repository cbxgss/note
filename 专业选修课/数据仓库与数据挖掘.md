# 1 引论

## 数据

-   数据库-关系表
-   数据矩阵
-   文本数据 document data
-   交易数据 transaction data

## 1.1 & 1.2 数据挖掘 Data Mining

-   **数据和信息**
    -   数据：无组织
    -   信息：数据经过处理，有组织有结构的
-   **数据挖掘**: 数据 $\rightarrow$ 信息
    从大量数据中提取interesting (non-trivial非微小的, implicit隐含的, previously unknown事先未知的, potentially useful)
-   Knowledge Discovery(**KDD**) Process
    -   **数据清洗** data cleaning
    -   **数据选择** data warehouse
    -   **数据挖掘** data mining
    -   **模式(规律)评价** pattern evaluation
    -   **知识** knowledge

## 1.4 数据挖掘的类型

-   **数据挖掘的两大类任务**
    -   **预测性方法**：用已知变量信息**预测**未知或未来
    -   **描述性方法**：找到**人类可理解的模式**来描述数据

-   **Mining Task**
    -   **关联规则** Associate rule
    -   **分类** classification
    -   **聚类** clustering

## 1.5 DM和其他学科的关系

## 1.6 应用

## 1.8 挑战

-   noise
-   uncertainty
-   imcompleteness(缺失数据)
-   趋势
    -   大数据
    -   高维度
    -   heterogeneous
    -   open

# 2 认识数据

数据选择和数据挖掘 data warehouse and data mining

## 2.1 数据对象与属性类型

data objects and attribute types

-   **数据对象 data object**(database的一行): 代表一个entity
-   **属性 attribute**(database的一列)
    -   **离散属性与连续属性 discrete and continuous**
        -   **标称属性 nominal**: (符号或事物的名称、类别、编码等)，不必有顺序，非定量(数学操作无意义)
        -   **二元属性 Binary**，只有2个值的标称属性
            -   **对称二元属性 Symmetric binary**: 两个值权重相同，eg: 性别
            -   **非对称二元属性 Asymmetric binary**: eg: 有病/没病。用1对重要结果编码，另一个用0

        -   **序数属性 Ordinal** 有序的属性，但是相邻的差距未知 (如A+等成绩等级，有众数和中位数，没有mean)

    -   **数值属性 Numeric** 数值的，定量的，有尺度
        -   **区间标度属性 interval-scale** (没有真正意义的0)，eg: 摄氏度
        -   **比率标度属性 ratio-scale**: 有0，eg: 开氏温标


## 2.2 基本统计描述

basic statistical descriptions of data

-   **中心趋势度量**
    -   均值 $mean$
        -   加权算术平均: 给每个值一个加权
        -   截尾平均: 丢弃极端值
    -   中位数 $median$
    -   众数 $mode$
    -   中列数 $midrange= \frac{max+min}2$
    
-   **度量数据散布**
    
    -   极差 $range = max - min$
    -   分位数 $quantiles$: $f\%$ 的计算如下
        -   排序
        -   $(n+1)\times f\%=j+q$
            -   $j$: 整数部分
            -   $q$: 小数部分
        -   $f\%=$
            -   如果 $q = 0$: $x(j)$
            -   如果 $q \neq 0$:
                -   直接平均: $\frac{x(j) + x(j+1)}{2}$
                -   加权平均: $(1-q) \times x(j) + q \times x(j+1)$
    -   四分位数 $quartiles$ $Q_1, Q_2, Q_3$，其中 $Q_2 = median$
    -   四分位极差 $interquartile range$: $IQR = Q_3-Q_1$
    
-   **盒图**分析
    
    -   五数概括: min, $Q_1$, median, $Q_3$, max
    -   盒子两端是$Q_1$和$Q_3$
    -   盒子中间的线: $Q_2$，即中位数
    -   胡须
        -   当 $max, min$ 超过 $Q_1, Q_3$ 不到 $1.5*IQR$ 时，延伸到 $max, min$
        -   否则，延伸到 $1.5*IQR$ 内的最极端观测点，剩下的变成野点
    -   野点: 盒子外单独画
    
    <img src="%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211115185345237.png" alt="image-20211115185345237" style="zoom: 80%;" />
    
-   **方差和标准差**

    -   sample: $s^2=\frac{1}{n-1}\sum\limits_{i=1}^n (x_i - \mathop{x}\limits^-)^2 = \frac{1}{n-1}(\sum\limits_{i=1}^nx_i^2-n{\mathop{x}\limits^-}^2)$
    -   population: $\sigma^2 = \frac{1}{N}\sum\limits^n_{i=1}(x_i-\mu)^2=\frac{1}{N}\sum\limits_{i=1}^nx_i^2-\mu^2$


## 2.3 数据的基本统计描述的图形显示

data visualization / graphic displays

-   **分位数图** **quantile plot**: 单变量

    ![image-20211115185640317](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211115185640317.png)

-   **分位数-分位数图** **quantile-quantile(q-q) plot**: 2变量求分位数，将其画在2维空间内

    ![image-20211115185703993](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211115185703993.png)

    -   如果两个变量样本数量相同，排序后，点 $(x_i,y_i)$ 的坐标就是排序的第 $i$ 个样本的值
    -   如果不同，则需要插值
    -   黑线是 $y=x$

-   **直方图分析** **histogram**

    ![image-20211115190523653](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211115190523653.png)

    -   x: 数值
    -   y: 该数值的频率

-   **散点图** **scatter plot**: 观察双变量数据的有用的分法

    ![image-20211115190622428](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211115190622428.png)

## 2.4 数据的相似性和相异性

measuring data similarity and dissimilarity

-   **相似性**: $sim(i, j) \in [0, 1]$， 0表示不相似

    -   对两个对象的相似程度进行度量

-   **相异性**:  $[0, +\infty]$

    -   **数据矩阵** data matrix

        -   一行为一个对象
        -   一列为一个属性

        $$
        \begin{bmatrix} x_{11}&x_{12}&x_{13}\\ x_{21}&x_{22}&x_{23}\\ x_{31}&x_{32}&x_{33}\\ \end{bmatrix}
        $$

        

    -   **相异性矩阵** dissimilarity matrix

        -   对象之间的相异性

        $$
        \begin{bmatrix} 0\\ d(2,1)&0\\ d(3,1)&d(3,2)&0\\ \end{bmatrix}
        $$

-   各种类型属性的邻近性度量

    -   **标称属性**

        -   方法1
            -   $d(i,j) = 1-\frac mp$，即不匹配的属性比例
                -   m: match的标称属性的个数
                -   p: 总个数
            -   $sim(i, j) = 1 - d(i, j) = \frac{m}{p}$
        -   方法2: 将标称属性转化为多个二元属性(独热编码，一个值变成一个二元属性)，使用列联表计算
    
    -   **二元属性**: 列联表，里面是 $(i,j)$ 的个数
    
        | i \ j |   1   |   0   |  sum  |
        | :---: | :---: | :---: | :---: |
        |   1   |   q   |   r   | q + r |
        |   0   |   s   |   t   | s + t |
        |  sum  | q + s | r + t |   p   |
    
        -   **对称二元变量**： $d(i,j) = \frac{r+s}{q+r+s+t}$​
        -   **非对称二元变量**：$d(i,j) = \frac{r+s}{q+r+s}$
            -   $sim_{Jaccard}(i,j) = \frac{q}{q+r+s}$
    
    -   **数值属性**
    
        -   距离
    
            -   非负
            -   同一: $d(i,i)=0$
            -   对称: $d(i,j)=d(j,i)$
            -   三角不等式: $d(i,j)<=d(i,k)+d(k,j)$
            -   eg
                -   闵可夫斯基距离: $d(i, j) = \sqrt[h]{\sum\limits_k |x_{ik} - x_{jk}|^k}$
                -   欧式距离: 直线距离，即 $h=2$
                -   曼哈顿距离: $|x|+|y|+|z|$，即 $h=1$
                -   上确界距离: $\max\limits_k |x_{ik} - x_{jk}|$，即 $h \rightarrow \infty$
    
            -   PS: 当p个属性的取值尺度不同时，标准化是必须的
    
    -   **有序属性**
    
        -   相异性
            -   用其rank $r_i$ 代替属性取值
            -   映射到[0, 1]: $z_{if}=\frac{r_{if}-1}{M_f-1}$
            -   然后随便用一个距离得到 $d(i,j)$
    
        -   $sim(i,j) = 1 - d(i,j)$
    
    -   **混合类型属性**
    
        -   用加权组合各种**类型**的属性
            $$
            d(i,j)=\frac{\sum_{f=1}^p \delta_{ij}^{(f)}d_{ij}^{(f)}}{\sum_{f=1}^p \delta_{ij}^{(f)}}
            $$
    
            -   $\delta_{ij}^{(f)}=0$: 有下面两种情况，其他为1
                -   $x_{if}$ 或 $x_jf$ 有缺失
                -   $x_{if}=x_{jf}=0$ 且 $f$ 是非对称二元属性
            -   $d_{ij}^{(f)}$: 针对属性 $f$，对象 $i$ 和 $j$ 之间的相异性
                -   其中如果 $f$ 是数值的，需要归一化: $d_{ij}^{(f)}=\frac{|x_{if}-x_{jf}|}{max_hx_{hf} - min_hx_{hf}}$
    
    ![image-20211118233152721](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118233152721.png)

![image-20211118232716323](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118232716323.png)

# 3 数据预处理

-   现实数据很脏
-   两种解决方法
    -   检测并矫正数据的质量
    -   使用算法来容忍低质量数据

## 3.1 概述

### 3.1.1 数据质量

-   **准确性** accuracy
-   **完整性** completeness
-   **一致性** consistency
-   时效性
-   可信性
-   可解释性

### 3.1.2 预处理的主要任务

-   **数据清洗 clean**
    -   缺失值
    -   噪点，野点
    -   不一致性
-   **数据集成 integration**
    -   多个数据源来的数据，需要集成
    -   数据库，立方体，文件
-   **数据归约 reduction**
    -   纬度约简
    -   数据压缩
-   **数据变换和离散化 transformation and discretization**
    -   规范化
    -   概念分成产生

## 3.2 数据清洗

-   **缺失值**

    -   忽略
    -   人工填补
    -   自动填补
        -   常数
        -   均值
        -   同一种类别的属性的均值
        -   最有可能的值

-   **噪声**

    -   **分箱**: 考察数据周围的值来光滑有序数据值。先排序，后分箱，然后光滑

        -   用均值光滑
        -   用中位数光滑
        -   用箱边界光滑

        <img src="%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211116104924846.png" alt="image-20211116104924846" style="zoom:67%;" />

    -   **回归**: 用其他属性预测另一个属性

    -   **离群点分析**: 用聚类来检测离群点

## 3.3 数据集成

解决冗余性和不一致性

-   **实体识别**: 等价实体如何匹配
-   **冗余和相关分析**: 用相关分析来检测冗余
    -   标称数据的 $x^2$ 相关检验
    -   数值数据的相关系数 $r_{A,B} = \frac{\sum\limits^n_{i=1}(a_i-\mathop{A}\limits^-)(b_i-\mathop{B}\limits^-)}{n\sigma_A\sigma_B} = \frac{\sum\limits_{i=1}^n(a_ib_i) - n\mathop{A}\limits^- \mathop{B}\limits^-}{n\sigma_A\sigma_B}$
        -   $r_{A,B} > 0$: 正相关
        -   $r_{A,B} < 0$: 负相关
        -   $r_{A,B}=0$: 不相关
    -   数值数据的协方差 $Cov(A,B) = E((A - \mathop{A}\limits^-)(B - \mathop{B}\limits^-))$
        -   $r_{A,B} = \frac{Cov(A,B)}{\sigma_A\sigma_B}$
        -   $Cov(A,B) = E(AB) - \mathop{A}\limits^- \mathop{B}\limits^-$
-   **元组重复**
-   **数据值冲突的检测与处理**
    -   不同来源属性值不一样
    -   不同的描述方式或度量尺度

## 3.4 数据归约

-   **纬数约简 dimensionality**
    -   小波变换
    -   PCA 主成分分析
    -   属性子集选择
-   **数据约简 numerosity**
    -   参数化方法
        -   假设数据满足某种模型
        -   估计模型的参数
        -   存储参数，丢弃数据(除了野点)
    -   非参数化方法
        -   直方图: 对数据进行分箱，然后存储均值或和
            -   划分规则
                -   等宽: 每个箱子宽相同。即每个盒子的max-min相同
                -   等频: 等深度，每个箱子里的个数近似
            -   比盒图包含更多信息
        -   聚类
        -   采样: 获得一个小的样本集来代表整个数据集
            -   采样方式
                -   随即采样 SRS
                -   无放回的采样 SRSWOR
                -   有放回的采样 SRSWR
    -   数据立方体聚集
-   **数据压缩 data**

## 3.5 数据变换和离散化

-   **数据变换**: 某个属性的数值映射到一个新的集合

-   **规范化**

    -   最大最小规范化

        -   规范到 $[new\_min, new\_max]$
            $$
            new\_v = \frac{v-min}{max-min}(new\_max-new\_min) + new\_min
            $$

    -   z分数规范化

        -   $new\_v = \frac{v - \mu_A}{\sigma_A}$
        -   降低了野点的影响

    -   小数定标规范化

        -   $new\_v = \frac{v}{10^j}$，$j$ 是让 $new\_v$ 都小于 $1$ 的最小整数

-   **离散化**

    -   分箱: 与数据清洗中类似
    -   直方图: 与数据归约中类似
    -   聚类，决策树，相关分析

-   **由标称属性产生概念分层**

    -   用户或专家显式地说明属性的部分序
    -   分析属性的取值个数情况: 多不同取值的在最底层

# 6 挖掘频繁模式、关联和相关性

## 6.1 基本概念

-   **频繁模式** **frequent pattern**
    -   **项集** **Itemset**
    -   **k项集 k-itemset**: $C_k$
    -   **支持度**
        -   **absolute** $\sigma(X)$: 项集出现的次数
        -   **relative** $s(X)$: 项集出现的频率
    -   **频繁项集 Frequent Itemset**: $L_k$: $支持度 \geq threshold$
-   **关联规则** **association rule** $X \rightarrow Y$
    -   $X, Y \sub I$
    -   $X \cap Y = \empty$

-   **支持度support**: $s(XY) = \frac{XY项集频率}{all记录}$，即$P(XY)$
-   **置信度 confidence**: $c(X \Rightarrow Y) = \frac{XY项集频率}{X项集的频率}$，即$P(Y|X)$
-   **强规则**: $s \geq minsup threshold \and c \geq minconf threshold$，计算:
    -   找频繁项集 (支持度 相同 且 高)
    -   通过将频繁项集二分，产生高置信度的规则

## 6.2 频繁项集的产生

Frequent Itemset Mining Methods

-   减少候选集的数量

    -   **Apriori principle准则**: 如果一个项集不频繁，则有这个项集的所有项集都不频繁

    ```c++
    产生1频繁项集;
    k = 1;
    while (有新频繁项集产生) {
        从k频繁自连接得到k+1频繁的候选集;
        对包含k不频繁项集的 k+1频繁项集 剪枝;
        计算 k+1项集候选集 的支持度;
        去掉不频繁, 得到 k+1 频繁项集;
    }
    ```

-   规则的产生

    -   从同一个项集中产生的规则的 c 反单调
    -   $c(ABC \rightarrow D)\geq c(AB \rightarrow CD)\geq c(A \rightarrow BCD)$
    -   对每一个项集都分析

## 6.3 哪些频繁模式interesting

-   **强规则不一定是有趣的**

-   **从关联分析到相关分析**
    
    -   **提升度**: 
        $$
        lift(A,B)= \frac{c(A \Rightarrow B)}{s(B)} = \frac{P(AB)}{P(A)P(B)}=\frac{P(A|B)}{P(A)}
        $$
    
        -   $lift(A,B) = 1$: 独立
        -   $lift(A,B) < 1$: 负相关
        -   $lift(A,B) >1$: 正相关

![image-20211118233010592](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118233010592.png)

![image-20211118232604405](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118232604405.png)

<img src="%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118232625503.png" alt="image-20211118232625503" style="zoom:50%;" />

# 8 分类

## 8.1 基本概念

-   记录的分类
    -   训练集 training set
    -   测试集 test set
-   预测任务
    -   分类
    -   数值预测

-   分类步骤
    -   模型构建
    -   模型使用
        -   估计模型的准确率

## 8.2 决策树

decision tree induction

-   一些定义
    -   根结点 Root node
    -   内部结点 internal node: 一个属性上的测试
    -   分支 branch: 测试的一个输出
    -   叶结点 leaf node: 存放一个类标号
    
-   **决策树的求解**: 使用贪心算法，每次选择的划分都是局部最优

    ```c++
    // 输入:
    //	attribute_list: 候选属性的集合
    //	attribute_selection_method 划分方法 = 分类属性 + 分裂点 + 划分子集
    node f (D, attribute_list) {
    	创建一个结点N;
        if (D中都是C类) 返回 N 作为叶结点，以类C标记;
        if (attribute_list 为空) 返回 N 作为叶结点，标记为多数的类;
    
        splitting_criterion = attribute_selection_method();
        用 splitting_criterion 标记 N;
    
        if (splitting_criterion 是离散值 && 允许多分支划分) {
            // 删除分裂属性，因为这个属性没用了
            attribute_list = attribute_list - splitting_attribute;
        }
    
        // 对每个划分产生子树
        for (splitting_criterion 的每个输出 j) {
            D_j = D中满足输出j的数据元组的集合;
            if (D_j 为空) {
                加一个树叶到 N，标记为 D 中的多数类;
            } else {
                加一个 f(D_j, attribute_list) 返回的结点到N;
            }
        }
        
        返回N;
    }
    ```

-   **属性的选择**
    
    -   **结点的不纯度**
        
        -   用**信息熵 Entropy**表示
            -   $Ent(D) = -\sum_ip_ilog_2p_i$
                -   $D$: 样本集合
                -   $p_i$: 第 $i$ 类占的比例
            -   当 $p_1=p_2=...=p_c=\frac1 c$ 时，信息熵最大
        -   熵越大，不纯度越大
        
    -   **ID3**: **信息增益**： 用属性 $A$ 划分得到的信息增益
        
        -   $Gain(D,A)=Info(D)-Info_A(D)$
            -   $Info(D) = Ent(D)$
            -   $Info_A(D) = \sum_{j=1}^V\frac{|D^j|}{|D|}Ent(D^v)$
                -   $V$: $A$ 可能的取值
                -   $j$: $A$ 的一种取值
        -   对可取值数目较多的属性有偏好
        
    -   **c4.5**: **增益率**
        
        
        $$
        Gainratio(D,A)=\frac{Gain(D,A)}{SplitInfo_A(D)}
        $$
        
        $$
        SplitInfo_A(D)=-\sum_{j=1}^V\frac{|D^j|}{|D|}log_2\frac{|D^j|}{|D|}
        $$
    
    -   **gini指数 CART**: 只分为两叉
        
        -   划分之前:
            $$
            gini(D)=1-\sum_{j=1}^np_j^2
            $$
            
        -   用A二分后的gini指数:
            $$
            gini_A(D)=\frac{|D_1|}{|D|}gini(D_1)+\frac{|D_2|}{|D|}gini(D_2)
            $$
            
    
-   

![image-20211118233114942](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118233114942.png)

![image-20211118232657184](%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211118232657184.png)

## 8.5 模型评估和选择

-   数据集划分方法
    -   保持法: 随即分成两个独立的集合
    -   随机法: 重复k次保持法，计算k次精度的均值
    -   交叉验证法(特殊情况: 留一法)
    
-   模型评价
    -   混淆矩阵 \[实际\]\[预测\]
    
        | 实际 \ 预测 | yes  |  no  |      |
        | :---------: | :--: | :--: | :--: |
        |     yes     |  TP  |  FN  |  P   |
        |     no      |  FP  |  TN  |  N   |
        |             |  P'  |  N'  | ALL  |
    
    -   基本指标
        -   准确率: 预测对了的
            $$
            Accuracy = \frac{TP+TN}{ALL}
            $$
        
        -   错误率: 预测错了的
            $$
            Rrrorrate=\frac{FP+FN}{ALL}
            $$
        
    -   类别不平衡问题
        -   敏感度，召回率，真正例率: 实际yes中，预测yes
            $$
            recall = Sensitivity = \frac{TP}{P}
            $$
        
        -   特效性，真负例率: 实际no中，预测no
            $$
            Secificity=\frac{TN}{N}
            $$
        
        -   ROC曲线: 横坐标特效性，纵坐标召回率，越靠左上，面积越大越好
            
        -   精度: 预测yes中，实际yes
            $$
            precision=\frac{TP}{TP + FP}
            $$
        
        -   $F$ 分数 / $F_1$ 分数
            $$
            \frac{2 * p * r}{p + r}
            $$
        
            -   p是精度，r是召回率
        
        -   $F_{\beta}$ 分数
            $$
            \frac{(1+\beta^2) \cdot p \cdot r}{\beta^2 \cdot p + r}
            $$
            

## 9.5 KNN

-   混合类型的属性加权来计算距离
-   基本思想: 最近的k个的类别相同
-   预测过程: 看被预测样本最近的k个训练样本的大多数的类别

## 9.3 支持向量机SVM

-   模型: $f(x, w, b) = sign(wx + b)$
-   步骤
    -   对所有巡逻数据正确分类
    -   最大化间隔 $M = \frac{2}{|w|} \; \; s.t. \; \; y_if(wx_i+b) > 0 \and min(y_if(wx_i+b)=1)$
        -   相当于最小化 $\frac{1}{2}|w|$
-   非线性情况
-   核相似性
-   多类问题

## 9.2 神经网络NN

-   神经元模型
-   感知器
    -   xor问题
-   神经网络
    -   两层神经网络可以表示任意布尔函数
    -   两层神经网络可以逼近任意连续函数
    -   三层神经网络可以逼近任意函数
-   深度学习

# 10聚类

## 10.1 基本概念

-   准则
    -   **粘连性**: 高的类内相似性
    -   **区分性**: 低的类间相似性

-   **簇的类型**
    -   **完全可分的簇**: 一个簇里的一个点，与簇内最远的点的距离，都小于簇外的点
    -   **基于中心的簇**: 簇内的一个点，距离簇中心的距离，小于簇外的点
    -   **基于连通性的簇**: 簇内的一个点，距离簇中的最近点的距离，小于簇外的点
    -   **基于密度的簇**: 簇是一团高密度的点，与其他簇之间，有低密度的点的区域分隔
-   聚类的类型
    -   **基于划分的聚类**: 不同的集合互相不重叠
    -   **分层聚类**: 得到的簇有层级结构
    -   **基于密度的方法**
    -   **基于网格的方法**

## 10.2 划分方法

-   算法目标: 把n个对象划分到k个簇，使每个点到聚类中心的距离平方和最小

    $$
    SSE=\sum_{i=1}^k\sum_{p \in C_i}(p-c_i)^2
    $$
    适用于发现球形的簇
    
-   k-means

    ```c++
    // 输入: k簇的数目 D:包含n个对象的数据集
    // 输出: k个簇的集合
    
    从D中任意选k个对象作为中心;
    while(发生变化) {
        根据簇中对象的均值，把每个对象分配到最相似的簇;
        更新簇均值;
    }
    ```

    -   一些问题
        -   簇个数
        -   不同的初始点会导致不同的簇
        -   对野点敏感

## 10.3 层次聚类方法

-   **自底向上**: 迭代的合并蔟
    -   计算簇间的邻近性
        -   min距离
        -   max距离
        -   mean距离
        -   中心距离

-   **自顶向下**: 递归的划分蔟

## 10.4 基于密度的聚类方法

-   优点
    -   可以发现任意形状的蔟
    -   对噪声具有抵抗力
-   基本概念
    -   $D$: 样本集
    -   两个参数
        -   $eps$: 邻域的半径
        -   $MinPts$: 阈值

    -   样本 $p$ 的**邻域**: $N_{eps}(p)=\{q\;belongs\;to\;D\;|\;dist(p,q) < eps\}$
    -   $q$ **直接密度可达** $p$
        -   $p\in N_{eps}(q)$
        -   $|N_{eps}(q)| \geq MinPts$，即 $q$ 是核心对象

    -   $q$ **密度可达** $p$: 点 $q$ 通过多次直接密度可达可以碰到点 $p$
    -   $q$ 和 $p$ **密度相连**: 点$ q,p$ 之间存在一个点 $o$，从 $o$ 到 $p, q$ 都密度可达

-   主要方法: DBSCAN

```c++
// 输入:
	// D: n个对象的数据集
	// \epsilon: 半径
	// MinPts: 邻域密度阈值
// 输出: 基于密度的簇的集合

标记所有点为unvisited;
while(有unvisitied) {
    随机选一个unvisited对象p;
    标记p为visited;

    if (p的\epsilon邻域有MinPts个对象) {
        创建一个新簇C，把p加入C;
        N = p的\epsilon邻域中对象的集合;
        for (pp in N) {
            // 扩展N
            if (pp unvisited) {
                pp visited;
                if (pp的\epsilon邻域有MinPts个点) {
                    把pp邻域的点加入N;
                }
            }
            // 加入簇
            if (pp不属于任何簇) pp加入C;
        }
        输出C;
    } else {
        p为噪声;
    }
}
```

## 10.6 聚类评估

-   外在方法
    -   用真实类别信息判断
    
    -   $JC = \frac{a}{a+b+c}$
    
        |        | 相同簇 | 不同簇 |
        | :----: | :----: | :----: |
        | 相同类 |   a    |   b    |
        | 不同类 |   c    |   d    |
    
    -   其他标准
    
        -   $RI = \frac{a+d}{a+b+c+d}$
        -   $FMI=\sqrt{\frac a{a+b}* \frac{a}{a+c}}$
    
    -   越大越好
    
-   内在方法
    -   样本真实类别信息不可用
    -   **簇的凝聚性** $WSS=SSE=\sum_i \sum_{x \in C_i} (x - m_i)^2$ 越小越好
        -   $C_i$: 一个簇
    -   **簇的分离性** $BSS = \sum_i|C_i|(m_i-m)^2$ 越大越好
    
    <img src="%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.assets/image-20211119002226650.png" alt="image-20211119002226650" style="zoom:50%;" />
    
    -   $WSS + BSS = constant = TSS = $ 每个点到所有点的均值的距离平方和

